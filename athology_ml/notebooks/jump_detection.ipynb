{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jump_detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEVUAjzPpVKc"
      },
      "source": [
        "The following simply prevents colab from exiting the session too quickly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzYQYEBnXB69"
      },
      "source": [
        "%%javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(ClickConnect,60000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HPv9PR2paRO"
      },
      "source": [
        "Install the packages we need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUF5jsEM2L9K"
      },
      "source": [
        "!pip install -U keras-tuner\n",
        "!pip install -q tensorflow-model-optimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTUIx9mfpdtv"
      },
      "source": [
        "Here, you should upload your data for training and testing (note if the following cell fails, you can use the file uploader in the left-hand side panel of colab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-bNzAzyaiwD"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lcc5PPfXpiFC"
      },
      "source": [
        "Import the packages we need and fix the random seeds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E6Iq4okIcs2"
      },
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import random\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from sklearn import metrics\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from kerastuner import HyperModel\n",
        "import kerastuner\n",
        "from tensorflow.data import Dataset\n",
        "from typing import Optional\n",
        "\n",
        "SEED = 13370\n",
        "NUMPY_SEED = 1337\n",
        "TF_SEED = 133\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(NUMPY_SEED)\n",
        "tf.random.set_seed(TF_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcLW-Wklak0W"
      },
      "source": [
        "Use the built in function for creating a `tf.data.Dataset` from a collection of CSV files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bkdAW4rND47"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "NUM_TIMESTEPS = 128\n",
        "BUFFER_SIZE = 500\n",
        "\n",
        "# For how to write the glob patterns, see: https://www.tensorflow.org/api_docs/python/tf/io/gfile/glob\n",
        "# For documentation on the dataloader, see: https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset\n",
        "\n",
        "kwargs = {\n",
        "    \"batch_size\": 1,\n",
        "    \"label_name\": \"is_air\",\n",
        "    \"select_columns\": [\"x-axis (g)\", \"y-axis (g)\", \"z-axis (g)\", \"is_air\"],\n",
        "    \"header\": True,\n",
        "    \"num_epochs\": 1,                  # Will set num_epochs within model.fit()\n",
        "    \"shuffle\": False                  # Must be false to sample windows as they appear in the input\n",
        "}\n",
        "\n",
        "train_dataset = tf.data.experimental.make_csv_dataset(\"jump_detection/train/*.csv\", **kwargs)\n",
        "valid_dataset = tf.data.experimental.make_csv_dataset(\"jump_detection/valid/*.csv\", **kwargs)\n",
        "test_dataset = tf.data.experimental.make_csv_dataset(\"jump_detection/test/*.csv\", **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_oC7afisymN"
      },
      "source": [
        "Here, we perform a series of steps:\n",
        "\n",
        "- Reshape the data\n",
        "- Implement best practices for fast data loading\n",
        "- Batch the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrNwlGC71ZAP"
      },
      "source": [
        "def pack_features_vector(features, labels):\n",
        "  \"\"\"Pack the features into a single array.\n",
        "  \n",
        "  Adapted from: https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough#create_a_tfdatadataset\n",
        "  \"\"\"\n",
        "  features = tf.stack(list(features.values()), axis=1)\n",
        "  # Remove all but the leading dimension.\n",
        "  # We set up batching outside of this function.\n",
        "  features = tf.reshape(features, shape=(-1, ))\n",
        "  labels = tf.reshape(labels, shape=(-1,))\n",
        "  \n",
        "  return features, labels\n",
        "\n",
        "def squeeze_labels_vector(features, labels):\n",
        "  \"\"\"Remove the trailing dimension of labels.\"\"\"\n",
        "  labels = tf.squeeze(labels, axis=-1)\n",
        "  return features, labels\n",
        "\n",
        "def cache_shuffle_batch_prefetch(\n",
        "    dataset: Dataset, batch_size: int, shuffle: bool = False, buffer_size: Optional[int] = None\n",
        "):\n",
        "    \"\"\"Given a `dataset`, returns a new `dataset` which generates batches of size `batch_size`.\n",
        "    If `shuffle`, the data is shuffled by randomly choosing items from `buffer_size` number of\n",
        "    examples. Follows best practices for optimized data loading by caching and prefetching the data.\n",
        "\n",
        "    See the individual tf.data.Dataset methods for more details:\n",
        "\n",
        "    - [cache](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache)\n",
        "    - [shuffle](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle)\n",
        "    - [batch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch)\n",
        "    - [prefetch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch)\n",
        "    \"\"\"\n",
        "    dataset = dataset.cache()\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=buffer_size, reshuffle_each_iteration=True)\n",
        "    dataset = dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# Stack the features and labels of the dataset into tensors\n",
        "train_dataset = train_dataset.map(pack_features_vector, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "valid_dataset = valid_dataset.map(pack_features_vector, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.map(pack_features_vector, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Create the first batch dimension, which corresponds to timesteps\n",
        "train_dataset = train_dataset.batch(NUM_TIMESTEPS, drop_remainder=True)\n",
        "valid_dataset = valid_dataset.batch(NUM_TIMESTEPS, drop_remainder=True)\n",
        "test_dataset = test_dataset.batch(NUM_TIMESTEPS, drop_remainder=True)\n",
        "\n",
        "# Batching introduces an unnecessary trailing dimension in the labels, drop it\n",
        "train_dataset = train_dataset.map(squeeze_labels_vector, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "valid_dataset = valid_dataset.map(squeeze_labels_vector, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.map(squeeze_labels_vector, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Finally, we setup traditional batching, using a helper function that also implements best practices (e.g. caching, prefetching)\n",
        "train_dataset = cache_shuffle_batch_prefetch(train_dataset, batch_size=BATCH_SIZE, shuffle=True, buffer_size=BUFFER_SIZE)\n",
        "valid_dataset = cache_shuffle_batch_prefetch(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, buffer_size=BUFFER_SIZE)\n",
        "test_dataset = cache_shuffle_batch_prefetch(test_dataset, batch_size=BATCH_SIZE, shuffle=False, buffer_size=BUFFER_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzqVdDo5EdES"
      },
      "source": [
        "features, labels = next(iter(train_dataset))\n",
        "print(f\"Features are of shape (batch_size, timesteps, feature_dim): {features.shape}\")\n",
        "print(f\"Labels are of shape: {labels.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHduRTULujuu"
      },
      "source": [
        "Next, we will run several import preprocessing steps:\n",
        "\n",
        "1. Compute a resonable bias for our classifier (to favour the more popular class)\n",
        "2. Compute appropriate class weights in order to weight our loss function (to favor the least popular class)\n",
        "3. Compute additional features, like `sum(x, y, z) = x + y + z` and `norm(x, y, z) = sqrt(x^2 + y^2 + z^2)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr7smvspX3iT"
      },
      "source": [
        "all_features = []\n",
        "all_labels = []\n",
        "\n",
        "random_baseline_precision = tf.keras.metrics.Precision()\n",
        "random_baseline_recall = tf.keras.metrics.Recall()\n",
        "always_jumping_precision = tf.keras.metrics.Precision()\n",
        "always_jumping_recall = tf.keras.metrics.Recall()\n",
        "\n",
        "for features, labels in iter(train_dataset):\n",
        "    features = features.numpy()\n",
        "    labels = labels.numpy()\n",
        "\n",
        "    # Flatten the batch dimension\n",
        "    all_features.append(features.reshape(-1, features.shape[-1]))\n",
        "    all_labels.append(labels.reshape(-1))\n",
        "\n",
        "    # Compute baseline precision and recalls to compare against\n",
        "    random_prediction = np.random.randint(2, size=labels.shape)\n",
        "    all_ones_prediction = np.ones_like(labels)\n",
        "    \n",
        "    random_baseline_precision.update_state(labels, random_prediction)\n",
        "    random_baseline_recall.update_state(labels, random_prediction)\n",
        "    always_jumping_precision.update_state(labels, all_ones_prediction)\n",
        "    always_jumping_recall.update_state(labels, all_ones_prediction)\n",
        "\n",
        "all_features = np.concatenate(all_features, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "# Compute a more reasonable initial bias value for our classifier\n",
        "# and reasonable weights for our pos and neg classes for our loss function.\n",
        "pos = all_labels.sum()\n",
        "neg = all_labels.shape[0] - pos\n",
        "classifier_bias = np.log(pos/neg)\n",
        "\n",
        "# Compute the sum and norm of our accelerometer data.\n",
        "# Train a normalization layer on all features.\n",
        "sum_ = np.sum(all_features, axis=-1, keepdims=True)\n",
        "norm = np.linalg.norm(all_features, axis=-1, keepdims=True)\n",
        "all_features = np.concatenate((all_features, sum_, norm), axis=-1)\n",
        "normalizer = preprocessing.Normalization(-1)\n",
        "normalizer.adapt(all_features)\n",
        "\n",
        "print(f\"Classifier bias: {classifier_bias}\")\n",
        "print(f\"Precision of random baseline: {random_baseline_precision.result().numpy()}\")\n",
        "print(f\"Recall of random baseline: {random_baseline_recall.result().numpy()}\")\n",
        "print(f\"Precision of always jumping baseline: {always_jumping_precision.result().numpy()}\")\n",
        "print(f\"Recall of always jumping baseline: {always_jumping_recall.result().numpy()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx4ZusL5qChc"
      },
      "source": [
        "Here, we actually define and build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGBb0qpPUdaC"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "METRICS = [\n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "]\n",
        "\n",
        "class FeatureExtractor(layers.Layer):\n",
        "    \"\"\"Extracts additional features from our inputs, like sum, and norm.\"\"\"\n",
        "    def call(self, inputs):\n",
        "        sum_ = tf.math.reduce_sum(\n",
        "            inputs, axis=-1, keepdims=True, name=\"sum\"\n",
        "        )\n",
        "        norm = tf.norm(inputs, axis=-1, keepdims=True, name=\"norm\")\n",
        "        return tf.concat([inputs, sum_, norm], axis=-1)\n",
        "\n",
        "def build_model():\n",
        "    inputs = keras.Input(shape=(None, 3))\n",
        "    feature_extractor = FeatureExtractor()\n",
        "    conv_1d = layers.Conv1D(\n",
        "        filters=32,\n",
        "        kernel_size=3,\n",
        "        strides=1,\n",
        "        padding=\"causal\",\n",
        "        activation=\"relu\"\n",
        "    )\n",
        "    dropout = layers.Dropout(0.2)\n",
        "    lstm_1 = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.1))\n",
        "    lstm_2 = layers.Bidirectional(layers.LSTM(32, return_sequences=True, dropout=0.2))\n",
        "    dense = layers.Dense(\n",
        "        1,\n",
        "        activation=\"sigmoid\",\n",
        "        bias_initializer=tf.keras.initializers.Constant(value=classifier_bias)\n",
        "    )\n",
        "    reshape = layers.Reshape((-1,))\n",
        "\n",
        "\n",
        "    x = feature_extractor(inputs)\n",
        "    x = normalizer(x)\n",
        "    x = conv_1d(x)\n",
        "    x = dropout(x)\n",
        "    x = lstm_1(x)\n",
        "    x = lstm_2(x)\n",
        "    x = dense(x)\n",
        "    outputs = reshape(x)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(5e-4),\n",
        "        loss=keras.losses.BinaryCrossentropy(),\n",
        "        metrics=METRICS,\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX-j-dEIGs_-"
      },
      "source": [
        "model = build_model()\n",
        "model.summary()\n",
        "tf.keras.utils.plot_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtYJhgGR2Zwm"
      },
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        \"weights.{precision:.2%}-{recall:.2%}.h5\", save_best_only=False, monitor=\"val_loss\"\n",
        "    ),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd18CPtVWlme"
      },
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=20,\n",
        "    callbacks=callbacks,\n",
        "    validation_data=valid_dataset\n",
        ")\n",
        "\n",
        "pretrained_weights = \"pretrained_weights.tf\"\n",
        "model.save_weights(pretrained_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3l1NeojhCs6"
      },
      "source": [
        "loss, _, precision, recall, _ = model.evaluate(test_dataset)\n",
        "print(f\"Precision: {precision:.2%}\")\n",
        "print(f\"recall: {recall:.2%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qei8ShWMOA2"
      },
      "source": [
        "__OPTIONAL__: The following code will prune the model during a short fine-tuning phase. See https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide#prune_some_layers_sequential_and_functional for more details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wqXiRxefSql"
      },
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
        "from tensorflow.python.keras.layers.wrappers import Bidirectional\n",
        "\n",
        "no_prune = (FeatureExtractor, Normalization, Bidirectional)\n",
        "\n",
        "def apply_pruning(layer):\n",
        "  if not isinstance(layer, no_prune):\n",
        "    return tfmot.sparsity.keras.prune_low_magnitude(layer)\n",
        "  return layer\n",
        "\n",
        "\n",
        "model = build_model()\n",
        "model.load_weights(pretrained_weights)\n",
        "\n",
        "model_for_pruning = tf.keras.models.clone_model(\n",
        "    model,\n",
        "    clone_function=apply_pruning,\n",
        ")\n",
        "\n",
        "model_for_pruning.summary()\n",
        "\n",
        "callbacks = [\n",
        "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "]\n",
        "\n",
        "model_for_pruning.compile(\n",
        "    # Drop the learning rate by a full order of magnitude\n",
        "    optimizer=keras.optimizers.Adam(5e-5),\n",
        "    loss=keras.losses.BinaryCrossentropy(),\n",
        "    metrics=METRICS,\n",
        ")\n",
        "\n",
        "_ = model_for_pruning.fit(\n",
        "    train_dataset,\n",
        "    callbacks=callbacks,\n",
        "    # Train for a fraction of the original number of epochs\n",
        "    epochs=5,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONQH6oHjq-YS"
      },
      "source": [
        "Compare the performance of the model after pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE-Ce7CLgiIr"
      },
      "source": [
        "loss, _, precision, recall, _ = model_for_pruning.evaluate(test_dataset)\n",
        "print(f\"Precision: {precision:.2%}\")\n",
        "print(f\"Recall: {recall:.2%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYKjB1tJrDoO"
      },
      "source": [
        "Save the pruned model to disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEzNE2MSHTGU"
      },
      "source": [
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
        "model_for_export.save(\"jump_detection.tf\", include_optimizer=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}